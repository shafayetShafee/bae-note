[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bae-note",
    "section": "",
    "text": "Preface\nThis is sort of a note for myself while reading the book “Bayes Rules” and eventually most of the contents of this note are just a copy-paste from the book.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "ch06-approx-posterior.html",
    "href": "ch06-approx-posterior.html",
    "title": "1  Approximating the Posterior",
    "section": "",
    "text": "1.1 Grid Approximation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Approximating the Posterior</span>"
    ]
  },
  {
    "objectID": "ch06-approx-posterior.html#grid-approximation",
    "href": "ch06-approx-posterior.html#grid-approximation",
    "title": "1  Approximating the Posterior",
    "section": "",
    "text": "1.1.1 Beta-Binomial Example\n\\[\n\\begin{equation}\n\\begin{split}\nY|\\pi & \\sim \\text{Bin}(10, \\pi) \\\\\n\\pi   & \\sim \\text{Beta}(2, 2)  . \\\\\n\\end{split}\n\\end{equation}\n\\]\nWe can interpret \\(Y\\) here as the number of successes in 10 independent trials. Each trial has probability of success π where our prior understanding about \\(\\pi\\) is captured by a \\(Beta(2, 2)\\) model. Suppose we observe \\(Y = 9\\) successes.\n\n# Step 1: Define a grid of 6 pi values\ngrid_data &lt;- data.frame(pi_grid = seq(from = 0, to = 1, length = 6))\n\n# Step 2: Evaluate the prior & likelihood at each pi\ngrid_data &lt;- grid_data %&gt;% \n  mutate(\n    prior = dbeta(pi_grid, 2, 2),\n    likelihood = dbinom(9, 10, pi_grid),\n    unnormalized = prior * likelihood,\n    # Step 3: Approximate the posterior\n    posterior = unnormalized / sum(unnormalized)\n  )\n\nround(grid_data, 2)\n\n  pi_grid prior likelihood unnormalized posterior\n1     0.0  0.00       0.00         0.00      0.00\n2     0.2  0.96       0.00         0.00      0.00\n3     0.4  1.44       0.00         0.00      0.01\n4     0.6  1.44       0.04         0.06      0.18\n5     0.8  0.96       0.27         0.26      0.81\n6     1.0  0.00       0.00         0.00      0.00\n\n\n\n# Plot the grid approximated posterior\nggplot(grid_data, aes(x = pi_grid, y = posterior)) +\n  geom_point() +\n  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior))\n\n\n\n\n\n\n\n\n\n# now sampling from posterior\nset.seed(84735)\n\n# Step 4: sample from the discretized posterior\npost_sample &lt;- sample_n(grid_data, size = 10000, weight = posterior, replace = TRUE)\n\n\npost_sample %&gt;% \n  tabyl(pi_grid) %&gt;% \n  adorn_totals(\"row\")\n\n pi_grid     n percent\n     0.4    69  0.0069\n     0.6  1885  0.1885\n     0.8  8046  0.8046\n   Total 10000  1.0000\n\n\n\n# Histogram of the grid simulation with posterior pdf\nggplot(post_sample, aes(x = pi_grid)) + \n  geom_histogram(aes(y = after_stat(density)), color = \"white\") +\n  stat_function(fun = dbeta, args = list(11, 3)) + \n  lims(x = c(0, 1))\n\n\n\n\n\n\n\n\nWe can get better simulations by using a fine grid,\n\ntibble(\n  # Step 1: Define a grid of 101 pi values\n  pi_grid = seq(from = 0, to = 1, length = 101),\n  \n  # Step 2: Evaluate the prior & likelihood at each pi\n  prior = dbeta(pi_grid, 2, 2),\n  likelihood = dbinom(x = 9, size = 10, prob = pi_grid),\n  \n  # Step 3: Approximate the posterior\n  unnormalized = prior * likelihood,\n  posterior = unnormalized / sum(unnormalized)\n) -&gt; grid_data\n\n# Plot the grid approximated posterior\ngrid_data %&gt;% \n  ggplot(aes(x = pi_grid, y = posterior)) +\n  geom_point() + \n  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior))\n\n\n\n\n\n\n\n\n\nset.seed(84735)\n\n# Step 4: sample from the discretized posterior\nsample_n(grid_data, size = 10000, replace = TRUE, weight = posterior) %&gt;% \n  ggplot(aes(x = pi_grid)) +\n  geom_histogram(aes(y = after_stat(density)), color = \"white\", binwidth = 0.05) +\n  stat_function(fun = dbeta, args = list(11, 3)) +\n  lims(x = c(0, 1))\n\n\n\n\n\n\n\n\n\n\n1.1.2 Gamma Poisson Example\nLet \\(Y\\) be the number of events that occur in a one-hour period, where events occur at an average rate of \\(\\lambda\\) per hour. Further, suppose we collect two data points \\((Y_1, Y_2)\\) and place a \\(Gamma(3, 1)\\) prior on \\(\\lambda\\):\n\\[\n\\begin{equation}\n\\begin{split}\nY_i|\\lambda & \\stackrel{ind}{\\sim} \\text{Pois}(\\lambda) \\\\\n\\lambda   & \\sim \\text{Gamma}(3, 1)  . \\\\\n\\end{split}\n\\end{equation}\n\\]\n\n# Step 1: Define a grid of 501 lambda values\ngrid_data   &lt;- data.frame(lambda_grid = seq(from = 0, to = 15, length = 501))\n\n# Step 2: Evaluate the prior & likelihood at each lambda\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = dgamma(lambda_grid, 3, 1),\n         likelihood = dpois(2, lambda_grid) * dpois(8, lambda_grid))\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\n# Set the seed\nset.seed(84735)\n\n# Step 4: sample from the discretized posterior\npost_sample &lt;- sample_n(grid_data, size = 10000, \n                        weight = posterior, replace = TRUE)\n\npost_sample %&gt;% \n  ggplot(aes(x = lambda_grid)) +\n  geom_histogram(aes(y = after_stat(density)), color = \"white\", binwidth = 0.8) +\n  stat_function(fun = dgamma, args = list(13, 3)) + \n  lims(x = c(0, 15))\n\n\n\n\n\n\n\n\n\n\n1.1.3 Limitations of Grid Approximation\nFor models with lots of parameters, grid approximation suffers from curse dimensionality.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Approximating the Posterior</span>"
    ]
  },
  {
    "objectID": "ch06-approx-posterior.html#markov-chains-via-rstan",
    "href": "ch06-approx-posterior.html#markov-chains-via-rstan",
    "title": "1  Approximating the Posterior",
    "section": "1.2 Markov chains via rstan",
    "text": "1.2 Markov chains via rstan\n\nMCMC samples are not taken directly from the posterior pdf \\(f(\\theta|y)\\)\nMCMC samples are not even independent (since generated from a markov chain)\n\n\n\n\n\n\n\nMarkov Chain Monte Carlo\n\n\n\nMCMC simulation produce a sample of N dependent \\(\\theta\\) values \\(\\left(\\theta^{(1)}, \\theta^{(2)}, \\dots , \\theta^{(N)}\\right)\\) which are not drawn from posterior pdf.\n\n\n\n1.2.1 Beta Binomial Example\n\n# STEP 1: DEFINE the model\nbb_model &lt;- \"\n  data {\n    int&lt;lower = 0, upper = 10&gt; Y;\n  }\n  \n  parameters {\n    real&lt;lower = 0, upper = 1&gt; pi;\n  }\n  \n  model {\n    Y ~ binomial(10, pi);\n    pi ~ beta(2, 2);\n  }\n\"\n\n\n# STEP 2: SIMULATE the posterior\nbb_sim &lt;- stan(model_code = bb_model, data = list(Y = 9), \n               chains = 4, iter = 5000*2, seed = 84735)\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.1.0.2.5)’\nusing SDK: ‘MacOSX14.2.sdk’\nclang -arch x86_64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Users/ovee/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/Rcpp/1.0.12/5ea2700d21e038ace58269ecdbeb9ec0/Rcpp/include/\"  -I\"/Users/ovee/Documents/Bayesian/bae-note/renv/library/R-4.3/x86_64-apple-darwin20/RcppEigen/include/\"  -I\"/Users/ovee/Documents/Bayesian/bae-note/renv/library/R-4.3/x86_64-apple-darwin20/RcppEigen/include/unsupported\"  -I\"/Users/ovee/Documents/Bayesian/bae-note/renv/library/R-4.3/x86_64-apple-darwin20/BH/include\" -I\"/Users/ovee/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/StanHeaders/2.32.7/3e1bf18c6ab1dc0a4a139bf566f78bbe/StanHeaders/include/src/\"  -I\"/Users/ovee/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/StanHeaders/2.32.7/3e1bf18c6ab1dc0a4a139bf566f78bbe/StanHeaders/include/\"  -I\"/Users/ovee/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/RcppParallel/5.1.7/a45594a00f5dbb073d5ec9f48592a08a/RcppParallel/include/\"  -I\"/Users/ovee/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/rstan/2.32.6/8a5b5978f888a3477c116e0395d006f8/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Users/ovee/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/StanHeaders/2.32.7/3e1bf18c6ab1dc0a4a139bf566f78bbe/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/x86_64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Users/ovee/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/StanHeaders/2.32.7/3e1bf18c6ab1dc0a4a139bf566f78bbe/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Users/ovee/Documents/Bayesian/bae-note/renv/library/R-4.3/x86_64-apple-darwin20/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Users/ovee/Documents/Bayesian/bae-note/renv/library/R-4.3/x86_64-apple-darwin20/RcppEigen/include/Eigen/Core:19:\n/Users/ovee/Documents/Bayesian/bae-note/renv/library/R-4.3/x86_64-apple-darwin20/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include &lt;cmath&gt;\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.037 seconds (Warm-up)\nChain 1:                0.037 seconds (Sampling)\nChain 1:                0.074 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.038 seconds (Warm-up)\nChain 2:                0.036 seconds (Sampling)\nChain 2:                0.074 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.037 seconds (Warm-up)\nChain 3:                0.039 seconds (Sampling)\nChain 3:                0.076 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.037 seconds (Warm-up)\nChain 4:                0.036 seconds (Sampling)\nChain 4:                0.073 seconds (Total)\nChain 4: \n\n\n\nas.array(bb_sim, pars = \"pi\") %&gt;% head(4)\n\n, , parameters = pi\n\n          chains\niterations   chain:1   chain:2   chain:3   chain:4\n      [1,] 0.9346988 0.7839062 0.8030837 0.8744565\n      [2,] 0.7278147 0.8617083 0.7439518 0.8055458\n      [3,] 0.7427938 0.8617083 0.8466726 0.6413999\n      [4,] 0.7673911 0.8392448 0.8023575 0.6556735\n\n\nPlease remember that, these Markov chain values are NOT a random sample from the posterior and are NOT independent.\n\nbayesplot::mcmc_trace(bb_sim, pars = \"pi\", size = 0.1)\n\n\n\n\n\n\n\n\n# Histogram of the Markov chain values\nmcmc_hist(bb_sim, pars = \"pi\") +\n  yaxis_text(\"TRUE\") +\n  ylab(\"count\")\n# Density plot of the Markov chain values\nmcmc_dens(bb_sim, pars = \"pi\") + \n  yaxis_text(TRUE) + \n  ylab(\"density\")\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.2 Gamma Poisson Example\n\n# STEP 1: DEFINE the model\ngp_model &lt;- \"\n  data {\n    int&lt;lower = 0&gt; Y[2];\n  }\n  \n  parameters {\n    real&lt;lower = 0&gt; lambda;\n  }\n  \n  model {\n    Y ~ poisson(lambda);\n    lambda ~ gamma(3, 1);\n  }\n\"\n\n# STEP 2: SIMULATE the posterior\ngp_sim &lt;- stan(model_code = gp_model, data = list(Y = c(2,8)), \n               chains = 4, iter = 5000*2, seed = 84735)\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.1.0.2.5)’\nusing SDK: ‘MacOSX14.2.sdk’\nclang -arch x86_64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Users/ovee/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/Rcpp/1.0.12/5ea2700d21e038ace58269ecdbeb9ec0/Rcpp/include/\"  -I\"/Users/ovee/Documents/Bayesian/bae-note/renv/library/R-4.3/x86_64-apple-darwin20/RcppEigen/include/\"  -I\"/Users/ovee/Documents/Bayesian/bae-note/renv/library/R-4.3/x86_64-apple-darwin20/RcppEigen/include/unsupported\"  -I\"/Users/ovee/Documents/Bayesian/bae-note/renv/library/R-4.3/x86_64-apple-darwin20/BH/include\" -I\"/Users/ovee/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/StanHeaders/2.32.7/3e1bf18c6ab1dc0a4a139bf566f78bbe/StanHeaders/include/src/\"  -I\"/Users/ovee/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/StanHeaders/2.32.7/3e1bf18c6ab1dc0a4a139bf566f78bbe/StanHeaders/include/\"  -I\"/Users/ovee/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/RcppParallel/5.1.7/a45594a00f5dbb073d5ec9f48592a08a/RcppParallel/include/\"  -I\"/Users/ovee/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/rstan/2.32.6/8a5b5978f888a3477c116e0395d006f8/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Users/ovee/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/StanHeaders/2.32.7/3e1bf18c6ab1dc0a4a139bf566f78bbe/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/x86_64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Users/ovee/Library/Caches/org.R-project.R/R/renv/cache/v5/R-4.3/x86_64-apple-darwin20/StanHeaders/2.32.7/3e1bf18c6ab1dc0a4a139bf566f78bbe/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Users/ovee/Documents/Bayesian/bae-note/renv/library/R-4.3/x86_64-apple-darwin20/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Users/ovee/Documents/Bayesian/bae-note/renv/library/R-4.3/x86_64-apple-darwin20/RcppEigen/include/Eigen/Core:19:\n/Users/ovee/Documents/Bayesian/bae-note/renv/library/R-4.3/x86_64-apple-darwin20/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include &lt;cmath&gt;\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.036 seconds (Warm-up)\nChain 1:                0.036 seconds (Sampling)\nChain 1:                0.072 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.039 seconds (Warm-up)\nChain 2:                0.037 seconds (Sampling)\nChain 2:                0.076 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.037 seconds (Warm-up)\nChain 3:                0.037 seconds (Sampling)\nChain 3:                0.074 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.036 seconds (Warm-up)\nChain 4:                0.035 seconds (Sampling)\nChain 4:                0.071 seconds (Total)\nChain 4: \n\n\n\n# Trace plots of the 4 Markov chains\nmcmc_trace(gp_sim, pars = \"lambda\", size = 0.1)\n\n\n\n\n\n\n\n\n# Histogram of the Markov chain values\nmcmc_hist(gp_sim, pars = \"lambda\") + \n  yaxis_text(TRUE) + \n  ylab(\"count\")\n# Density plot of the Markov chain values\nmcmc_dens(gp_sim, pars = \"lambda\") + \n  yaxis_text(TRUE) + \n  ylab(\"density\")\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.3 Markov Chain Diagnostics\nSome visual and Numerical Diagnostics\n\nTrace plot\nParallel Chains\nEffective Sample Size\nAutocorrelation\nR-hat (\\(\\hat{R}\\))\n\n\n\n\n\n\n\nNote\n\n\n\nFor a stable MCMC sample, there shouldn’t be any pattern in the trace plot, only the random movement.\n\n\n\nmcmc_dens_overlay(bb_sim, pars = \"pi\") +\n  ylab(\"density\")\n\n\n\n\n\n\n\n\nHere, we observe that these four chains produce nearly indistinguishable posterior approximations. This provides evidence that our simulation is stable.\n\n\n\n\n\n\nEffective Sample Size Ratio\n\n\n\nThe number of independent samples it would take to produce an equivalent accurate posterior estimation.\nTypically the effective sample size ratio is less than 1, but if this ratio is less than 0.1, then we might be suspicious of the underlying markov chain.\n\n\n\n# Calculate the effective sample size ratio\nneff_ratio(bb_sim, pars = c(\"pi\"))\n\n[1] 0.3682844\n\n\n\n\n\n\n\n\nAutocorrelation\n\n\n\nStrong autocorrelation or dependence is a bad thing – it goes hand in hand with small effective sample size ratios, and thus provides a warning sign that our resulting posterior approximations might be unreliable.\n\n\nmcmc_trace(bb_sim, pars = \"pi\")\nmcmc_acf(bb_sim, pars = \"pi\")\n\n\n\n\n\n\n\n\n\n\nThe above bb_sim Markov chain is mixing quickly, i.e., quickly moving around the range of posterior plausible \\(\\pi\\) values, and thus at least mimicking an independent sample.\n\n\n\n\n\n\nFast vs slow mixing Markov chains\n\n\n\nFast mixing chains exhibit behavior similar to that of an independent sample: the chains move “quickly” around the range of posterior plausible values, the autocorrelation among the chain values drops off quickly, and the effective sample size ratio is reasonably large.\nSlow mixing chains do not enjoy the features of an independent sample: the chains move “slowly” around the range of posterior plausible values, the autocorrelation among the chain values drops off very slowly, and the effective sample size ratio is small.\n\n\nSo how to tune a slow mixing chain?\n\nRun a longer chain\nThinning\n\n\n\n\n\n\n\nR-hat\n\n\n\nWe want our parallel Markov chains to be consistent to each other. R-hat addresses this consistency by comparing the variability in sampled values across all chains combined to the variability within each individual chain.\nIdeally, R-hat \\(\\approx\\) 1, reflecting stability across the parallel chains. In contrast, R-hat \\(\\gt\\) 1 indicates instability, with the variability in the combined chains exceeding that within the chains. Though no golden rule exists, an R-hat ratio greater than 1.05 raises some red flags about the stability of the simulation.\n\n\n\nrhat(bb_sim, pars = 'pi')\n\n[1] 1.000005",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Approximating the Posterior</span>"
    ]
  },
  {
    "objectID": "ch06-soln.html",
    "href": "ch06-soln.html",
    "title": "2  Solutions to Chapter 06 Exercises",
    "section": "",
    "text": "2.1 Practice: Grid approximation\n# Load packages\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(rstan)\nlibrary(bayesplot)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Solutions to Chapter 06 Exercises</span>"
    ]
  },
  {
    "objectID": "ch06-soln.html#practice-grid-approximation",
    "href": "ch06-soln.html#practice-grid-approximation",
    "title": "2  Solutions to Chapter 06 Exercises",
    "section": "",
    "text": "Exercise 2.1 (Beta-Binomial grid approximation) Consider the Beta-Binomial model for \\(\\pi\\) with \\(Y | \\pi ∼ Bin (n , \\pi)\\) and \\(π ∼ Beta (3, 8)\\). Suppose that in \\(n = 10\\) independent trials, you observe \\(Y = 2\\) successes.\n\nUtilize grid approximation with grid values \\(π ∈ \\{0 , 0.25 , 0.5 , 0.75 , 1 \\}\\) to approximate the posterior model of \\(π\\) .\nRepeat part a using a grid of 201 equally spaced values between 0 and 1.\n\n\n\nSolution 2.1 (a). \n\ntibble(\n  # Define a grid of pi values\n  pi_grid = c(0, 0.25, 0.5, 0.75, 1),\n  \n  # Evaluate the prior and llk at each value of grid\n  prior = dbeta(pi_grid, 3, 8),\n  likelihood = dbinom(2, size = 10, prob = pi_grid),\n  unnormalized = prior * likelihood,\n  \n  # Calculate the posterior\n  posterior = unnormalized / sum(unnormalized)\n) %&gt;% \n  # Sample from posterior\n  sample_n(size = 10000, replace = TRUE, weight = posterior) %&gt;% \n  \n  # Plot the histogram of posterior simulation \n  ggplot(aes(x = pi_grid)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  # overlay the theoretical posterior for comparison\n  stat_function(fun = dbeta, args = list(5, 16)) +\n  xlim(c(0, 1))\n\n\n\n\n\n\n\n\n\n\nSolution 2.2 (b). \n\ntibble(\n  # Define a grid of pi values\n  pi_grid = seq(from = 0, to = 1, length = 201),\n  \n  # Evaluate the prior and llk at each value of grid\n  prior = dbeta(pi_grid, 3, 8),\n  likelihood = dbinom(2, size = 10, prob = pi_grid),\n  unnormalized = prior * likelihood,\n  \n  # Calculate the posterior\n  posterior = unnormalized / sum(unnormalized)\n) %&gt;% \n  # Sample from posterior\n  sample_n(size = 10000, replace = TRUE, weight = posterior) %&gt;% \n  \n  # Plot the histogram of posterior simulation \n  ggplot(aes(x = pi_grid)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  # overlay the theoretical posterior for comparison\n  stat_function(fun = dbeta, args = list(5, 16)) +\n  xlim(c(0, 1))\n\n\n\n\n\n\n\n\n\n\nExercise 2.2 (Gamma-Poisson grid approximation) Consider the Gamma-Poisson model for \\(\\lambda\\) with \\(Y_i|\\lambda \\sim Pois(\\lambda)\\) and \\(\\lambda ~ Gamma(20, 5)\\) Suppose you observe \\(n = 3\\) independent data points \\((Y_1, Y_2, Y_3) = (0, 1, 0)\\).\n\nUtilize grid approximation with grid values \\(\\lambda ∈ \\{0, 1, 2, \\dots, 8\\}\\) to approximate the posterior model of \\(\\lambda\\).\nRepeat part a using a grid of 201 equally spaced values between 0 and 8.\n\n\n\nSolution 2.3 (a). \n\ntibble(\n  lambda_grid = 0:8,\n  prior = dgamma(lambda_grid, 20, 5),\n  llk = dpois(0, lambda_grid) * dpois(1, lambda_grid) * dpois(0, lambda_grid),\n  unnormalized = prior * llk,\n  posterior = unnormalized / sum(unnormalized)\n) %&gt;% \n  sample_n(size = 10000, replace = TRUE, weight = posterior) %&gt;% \n  ggplot(aes(x = lambda_grid)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dgamma, args = list(21, 8)) +\n  xlim(c(0, 8))\n\n\n\n\n\n\n\n\n\n\nSolution 2.4 (b). \n\ntibble(\n  lambda_grid = seq(from = 0, to = 8, length = 201),\n  prior = dgamma(lambda_grid, 20, 5),\n  llk = dpois(0, lambda_grid) * dpois(1, lambda_grid) * dpois(0, lambda_grid),\n  unnormalized = prior * llk,\n  posterior = unnormalized / sum(unnormalized)\n) %&gt;% \n  sample_n(size = 10000, replace = TRUE, weight = posterior) %&gt;% \n  ggplot(aes(x = lambda_grid)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dgamma, args = list(21, 8)) +\n  xlim(c(0, 8))\n\n\n\n\n\n\n\n\n\n\nExercise 2.3 (Normal-Normal grid approximation) Consider the Normal-Normal model for \\(\\mu\\) with \\(Y_i \\sim N(\\mu, 1.3^2)\\) and \\(\\mu \\sim N(10, 1.2^2)\\). Suppose that on \\(n = 4\\) independent observations, you observe data \\((Y_1, Y_2, Y_3, Y_4) = ( 7.1 , 8.9 , 8.4 , 8.6)\\).\n\nUtilize grid approximation with grid values \\(\\mu ∈ \\{ 5 , 6 , 7 , \\dots , 15 \\}\\) to approximate the posterior model of \\(\\mu\\) .\nRepeat part a using a grid of 201 equally spaced values between 5 and 15\n\n\n\nSolution 2.5 (a). \n\ncalc_llk &lt;- function(obs, par, pdf_func, ...) {\n  llk &lt;- outer(obs, par, pdf_func, ...) |&gt; \n    apply(MARGIN = 2, FUN = prod)\n  \n  return(llk)\n}\n\n\ntibble(\n  mu_grid = 5:15,\n  prior = dnorm(mu_grid, 10, 1.2),\n  llk = calc_llk(c(7.1, 8.9, 8.4, 8.6), mu_grid, dnorm, sd = 1.3),\n  unnormalized = prior * llk,\n  posterior = unnormalized / sum(unnormalized)\n) %&gt;% \n  sample_n(size = 10000, replace = TRUE, weight = posterior) %&gt;% \n  ggplot(aes(x = mu_grid)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dnorm, args = list(8.64697, 0.5715)) +\n  xlim(c(5, 15))\n\n\n\n\n\n\n\n\n\n\nSolution 2.6 (b). \n\ntibble(\n  mu_grid = seq(5, 15, length = 201),\n  prior = dnorm(mu_grid, 10, 1.2),\n  llk = calc_llk(c(7.1, 8.9, 8.4, 8.6), mu_grid, dnorm, sd = 1.3),\n  unnormalized = prior * llk,\n  posterior = unnormalized / sum(unnormalized)\n) %&gt;% \n  sample_n(size = 10000, replace = TRUE, weight = posterior) %&gt;% \n  ggplot(aes(x = mu_grid)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dnorm, args = list(8.64697, 0.5715)) +\n  xlim(c(5, 15))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Solutions to Chapter 06 Exercises</span>"
    ]
  },
  {
    "objectID": "ch06-soln.html#practice-mcmc",
    "href": "ch06-soln.html#practice-mcmc",
    "title": "2  Solutions to Chapter 06 Exercises",
    "section": "2.2 Practice: MCMC",
    "text": "2.2 Practice: MCMC\n\nExercise 2.4 (MCMC with RStan: Steps 1 and 2) Use the given information to (1) define the Bayesian model structure, and (2) simulate the posterior using the correct RStan syntax. You don’t need to run the code, just provide the syntax.\n\n\\(Y|\\pi \\sim Bin(20, \\pi)\\) and \\(\\pi \\sim Beta(1, 1)\\) with \\(Y = 12\\).\n\\(Y|\\lambda \\sim Pois(\\lambda)\\) and \\(\\lambda \\sim Gamma(4, 2)\\) with \\(Y = 3\\).\n\\(Y|\\mu \\sim N(\\mu, 1^2)\\) and \\(\\mu \\sim N(0, 10^2)\\) with \\(Y = 12.2\\).\n\n\n\nSolution 2.7 (a). \n\nbb_model &lt;- \"\n  data {\n    int&lt;lower = 0, upper = 20&gt; Y;\n  }\n  \n  parameters {\n    real&lt;lower = 0, upper = 1&gt; pi;\n  }\n  \n  model {\n    Y ~ binomial(20, pi);\n    pi ~ beta(1, 1);\n  }\n\"\n\nbb_sim &lt;- stan(model_code = bb_model, data = list(Y = 12),\n               chains = 4, iter = 5000*2, seed = 84735)\n\n\n\nSolution 2.8 (b). \n\ngp_model &lt;- \"\n  data {\n    int&lt;lower = 0&gt; Y;\n  }\n  \n  parameters {\n    real&lt;lower = 0&gt; lambda;\n  }\n  \n  model {\n    Y ~ poisson(lambda);\n    lambda ~ gamma(4, 2);\n  }\n\"\n\ngp_sim &lt;- stan(model_code = gp_model, data = list(Y = 3),\n               chains = 4, iter = 5000*2, seed = 84735)\n\n\n\nSolution 2.9. \n\nnn_model &lt;- \"\n  data {\n    real Y;\n  }\n  \n  parameters {\n    real mu;\n  }\n  \n  model {\n    Y ~ normal(mu, 1);\n    mu ~ normal(0, 10);\n  }\n\"\n\nnn_sim &lt;- stan(model_code = nn_model, data = list(Y = 12.2),\n               chains = 4, iter = 5000*2, seed = 84735)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Solutions to Chapter 06 Exercises</span>"
    ]
  }
]